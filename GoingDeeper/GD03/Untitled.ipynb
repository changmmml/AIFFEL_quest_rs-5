{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7dc2771",
   "metadata": {},
   "source": [
    "## 8-2. CAM, Grad-CAM용 모델 준비하기 (1) 데이터셋 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052eab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import copy\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded94b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8068c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최초 수행시에는 다운로드가 진행됩니다. 오래 걸릴 수 있으니 유의해 주세요.  \n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'stanford_dogs',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.show_examples(ds_train, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f42be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.show_examples(ds_test, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362fdbc",
   "metadata": {},
   "source": [
    "## 8-3. CAM, Grad-CAM용 모델 준비하기 (2) 물체의 위치정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a87be1",
   "metadata": {},
   "source": [
    "## 8-4. CAM, Grad-CAM용 모델 준비하기 (3) CAM을 위한 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = ds_info.features[\"label\"].num_classes\n",
    "base_model = keras.applications.resnet50.ResNet50(\n",
    "    include_top=False,    # Imagenet 분류기  fully connected layer 제거\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling='avg',      # GAP를 적용  \n",
    ")\n",
    "x = base_model.output\n",
    "preds = keras.layers.Dense(num_classes, activation='softmax', use_bias=False,)(x)\n",
    "cam_model = keras.Model(inputs=base_model.input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677635dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392be4a",
   "metadata": {},
   "source": [
    "## 8-5. CAM, Grad-CAM용 모델 준비하기 (4) CAM 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_resize_img(input):\n",
    "    # Normalizes images: `uint8` -> `float32`\n",
    "    image = tf.image.resize(input['image'], [224, 224])\n",
    "    input['image'] = tf.cast(image, tf.float32) / 255.\n",
    "    return input['image'], input['label']\n",
    "\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=16):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img, \n",
    "        num_parallel_calls=2\n",
    "    )\n",
    "    ds = ds.batch(batch_size)\n",
    "    '''\n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    '''\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6995394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에 전처리와 배치처리를 적용합니다.\n",
    "ds_train_norm = apply_normalize_on_dataset(ds_train)\n",
    "ds_val_norm = apply_normalize_on_dataset(ds_test)\n",
    "\n",
    "# 구성된 배치의 모양을 확인해 봅니다. \n",
    "for input in ds_train_norm.take(1):\n",
    "    image, label = input\n",
    "    print(image.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43007f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cam_model = cam_model.fit(\n",
    "    ds_train_norm,\n",
    "    steps_per_epoch=int(ds_info.splits['train'].num_examples/16),\n",
    "    validation_steps=int(ds_info.splits['test'].num_examples/16),\n",
    "    epochs=2,\n",
    "    validation_data=ds_val_norm,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cam_model_path = os.getenv('HOME')+'/aiffel/model_weight/GD03/'\n",
    "\n",
    "cam_model.save_weights(cam_model_path+'cam_model1.h5')\n",
    "cam_model.save(cam_model_path+'cam_model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b705b0",
   "metadata": {},
   "source": [
    "## 8-6. CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커널 재시작 이후 실습을 위해, 이전 스텝의 코드를 모아서 한꺼번에 실행합니다.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import copy\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'stanford_dogs',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "def normalize_and_resize_img(input):\n",
    "    # Normalizes images: `uint8` -> `float32`\n",
    "    image = tf.image.resize(input['image'], [224, 224])\n",
    "    input['image'] = tf.cast(image, tf.float32) / 255.\n",
    "    return input['image'], input['label']\n",
    "\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=16):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img, \n",
    "        num_parallel_calls=2\n",
    "    )\n",
    "    ds = ds.batch(batch_size)\n",
    "    '''\n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    '''\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a0b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one(ds):\n",
    "    ds = ds.take(1)\n",
    "    sample_data = list(ds.as_numpy_iterator())\n",
    "    bbox = sample_data[0]['objects']['bbox']\n",
    "    image = sample_data[0]['image']\n",
    "    label = sample_data[0]['label']\n",
    "    return sample_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cam_model_path = os.getenv('HOME')+'/aiffel/class_activation_map/cam_model1.h5'\n",
    "cam_model = tf.keras.models.load_model(cam_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cam(model, item):\n",
    "    item = copy.deepcopy(item)\n",
    "    width = item['image'].shape[1]\n",
    "    height = item['image'].shape[0]\n",
    "    \n",
    "    img_tensor, class_idx = normalize_and_resize_img(item)\n",
    "    \n",
    "    # 학습한 모델에서 원하는 Layer의 output을 얻기 위해서 모델의 input과 output을 새롭게 정의해줍니다.\n",
    "    cam_model = tf.keras.models.Model([model.inputs], [model.layers[-3].output, model.output])\n",
    "    conv_outputs, predictions = cam_model(tf.expand_dims(img_tensor, 0))\n",
    "    conv_outputs = conv_outputs[0, :, :, :]\n",
    "    \n",
    "    # 모델의 weight activation은 마지막 layer에 있습니다.\n",
    "    class_weights = model.layers[-1].get_weights()[0] \n",
    "    \n",
    "    cam_image = np.zeros(dtype=np.float32, shape=conv_outputs.shape[0:2])\n",
    "    for i, w in enumerate(class_weights[:, class_idx]):\n",
    "        # conv_outputs의 i번째 채널과 i번째 weight를 곱해서 누적하면 활성화된 정도가 나타날 겁니다.\n",
    "        cam_image += w * conv_outputs[:, :, i]\n",
    "\n",
    "    cam_image /= np.max(cam_image) # activation score를 normalize합니다.\n",
    "    cam_image = cam_image.numpy()\n",
    "    cam_image = cv2.resize(cam_image, (width, height)) # 원래 이미지의 크기로 resize합니다.\n",
    "    return cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4554dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_image = generate_cam(cam_model, item)\n",
    "plt.imshow(cam_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cam_on_image(src1, src2, alpha=0.5):\n",
    "    beta = (1.0 - alpha)\n",
    "    merged_image = cv2.addWeighted(src1, alpha, src2, beta, 0.0)\n",
    "    return merged_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb372ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_image = item['image'].astype(np.uint8)\n",
    "cam_image_3channel = np.stack([cam_image*255]*3, axis=-1).astype(np.uint8)\n",
    "\n",
    "blended_image = visualize_cam_on_image(cam_image_3channel, origin_image)\n",
    "plt.imshow(blended_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587d220",
   "metadata": {},
   "source": [
    "## 8-7. Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7edfd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grad_cam(model, activation_layer, item):\n",
    "    item = copy.deepcopy(item)\n",
    "    width = item['image'].shape[1]\n",
    "    height = item['image'].shape[0]\n",
    "    img_tensor, class_idx = normalize_and_resize_img(item)\n",
    "    \n",
    "    # Grad cam에서도 cam과 같이 특정 레이어의 output을 필요로 하므로 모델의 input과 output을 새롭게 정의합니다.\n",
    "    # 이때 원하는 레이어가 다를 수 있으니 해당 레이어의 이름으로 찾은 후 output으로 추가합니다.\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(activation_layer).output, model.output])\n",
    "    \n",
    "    # Gradient를 얻기 위해 tape를 사용합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, pred = grad_model(tf.expand_dims(img_tensor, 0))\n",
    "    \n",
    "        loss = pred[:, class_idx] # 원하는 class(여기서는 정답으로 활용) 예측값을 얻습니다.\n",
    "        output = conv_output[0] # 원하는 layer의 output을 얻습니다.\n",
    "        grad_val = tape.gradient(loss, conv_output)[0] # 예측값에 따른 Layer의 gradient를 얻습니다.\n",
    "\n",
    "    weights = np.mean(grad_val, axis=(0, 1)) # gradient의 GAP으로 weight를 구합니다.\n",
    "    grad_cam_image = np.zeros(dtype=np.float32, shape=conv_output.shape[0:2])\n",
    "    for k, w in enumerate(weights):\n",
    "        # output의 k번째 채널과 k번째 weight를 곱하고 누적해서 class activation map을 얻습니다.\n",
    "        grad_cam_image += w * output[:, :, k]\n",
    "        \n",
    "    grad_cam_image = tf.math.maximum(0, grad_cam_image)\n",
    "    grad_cam_image /= np.max(grad_cam_image)\n",
    "    grad_cam_image = grad_cam_image.numpy()\n",
    "    grad_cam_image = cv2.resize(grad_cam_image, (width, height))\n",
    "    return grad_cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a82e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv5_block3_out', item)\n",
    "plt.imshow(grad_cam_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6139d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv4_block3_out', item)\n",
    "plt.imshow(grad_cam_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv3_block3_out', item)\n",
    "plt.imshow(grad_cam_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ca6ee",
   "metadata": {},
   "source": [
    "## 8-8. Detection with CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289657da",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc35777",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_image = generate_cam(cam_model, item)\n",
    "plt.imshow(cam_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(cam_image, score_thresh=0.05):\n",
    "    low_indicies = cam_image <= score_thresh\n",
    "    cam_image[low_indicies] = 0\n",
    "    cam_image = (cam_image*255).astype(np.uint8)\n",
    "    \n",
    "    contours,_ = cv2.findContours(cam_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = contours[0]\n",
    "    rotated_rect = cv2.minAreaRect(cnt)\n",
    "    rect = cv2.boxPoints(rotated_rect)\n",
    "    rect = np.int0(rect)\n",
    "    return rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebda71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = get_bbox(cam_image)\n",
    "rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = copy.deepcopy(item['image'])\n",
    "image = cv2.drawContours(image, [rect], 0, (0,0,255), 2)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rect의 좌표는 (x, y) 형태로, bbox는 (y_min, x_min, y_max, x_max)의 normalized 형태로 주어집니다. \n",
    "def rect_to_minmax(rect, image):\n",
    "    bbox = [\n",
    "        rect[:,1].min()/float(image.shape[0]),  #bounding box의 y_min\n",
    "        rect[:,0].min()/float(image.shape[1]),  #bounding box의 x_min\n",
    "        rect[:,1].max()/float(image.shape[0]), #bounding box의 y_max\n",
    "        rect[:,0].max()/float(image.shape[1]) #bounding box의 x_max\n",
    "    ]\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bbox = rect_to_minmax(rect, item['image'])\n",
    "pred_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "item['objects']['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6127e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(boxA, boxB):\n",
    "    y_min = max(boxA[0], boxB[0])\n",
    "    x_min= max(boxA[1], boxB[1])\n",
    "    y_max = min(boxA[2], boxB[2])\n",
    "    x_max = min(boxA[3], boxB[3])\n",
    "    \n",
    "    interArea = max(0, x_max - x_min) * max(0, y_max - y_min)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_iou(pred_bbox, item['objects']['bbox'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
